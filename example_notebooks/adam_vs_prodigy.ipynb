{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def git_repo_root():\n",
    "    # Run the 'git rev-parse --show-toplevel' command to get the root directory of the Git repository\n",
    "    try:\n",
    "        root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'], universal_newlines=True).strip()\n",
    "        return root\n",
    "    except subprocess.CalledProcessError:\n",
    "        # Handle the case where the current directory is not inside a Git repository\n",
    "        return None\n",
    "\n",
    "# Get the root directory of the Git repository\n",
    "git_root = git_repo_root()\n",
    "\n",
    "if git_root:\n",
    "    # Change the working directory to the root of the Git repository\n",
    "    os.chdir(git_root)\n",
    "    print(f\"Changed working directory to: {git_root}\")\n",
    "else:\n",
    "    print(\"Not inside a Git repository.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from diffusion import VPSDE\n",
    "from data import generate_mixture_gaussians\n",
    "\n",
    "epochs = 5000\n",
    "# Make sure our diffusion process actually diffuses the data\n",
    "data = generate_mixture_gaussians()\n",
    "num_steps = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import loss_function\n",
    "import torch\n",
    "from diffusion import match_dim\n",
    "from data import log_likelihood_mixture_gaussians_batch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from prodigyopt import Prodigy\n",
    "\n",
    "def train_score_network(dataloader, score_net, sde, epochs=epochs, bridge=False, optimizer_str='adam', learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Trains the score network\n",
    "\n",
    "    \"\"\"\n",
    "    optimizer = None\n",
    "    if optimizer_str == 'adam':\n",
    "        optimizer = torch.optim.Adam(score_net.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = Prodigy(score_net.parameters())\n",
    "    avg = 0\n",
    "    epoch_ll = []\n",
    "    ll_mean = []\n",
    "    ll_std = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for x_batch, in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(score_net, x_batch, sde, bridge=bridge)\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(score_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            avg += loss\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            samples = sde.backward_diffusion(score_net, data_shape=(1000, 2)).detach()\n",
    "            lls = log_likelihood_mixture_gaussians_batch(samples).numpy()\n",
    "            ll_mean.append(lls.mean())\n",
    "            ll_std.append(lls.std())\n",
    "            epoch_ll.append(epoch)\n",
    "\n",
    "\n",
    "        if (((epoch + 1) % 500 == 0 and epoch != 0) or epoch == epochs-1):\n",
    "            tqdm.write(f'Epoch: {epoch} and Loss: {avg/(8*1000)}')\n",
    "            avg = 0\n",
    "            # samples = sde.backward_diffusion(score_net, data_shape=(1000, 2))\n",
    "            # data = x_batch.detach().numpy()\n",
    "            # samples_np = samples.detach().numpy()\n",
    "            # lls = log_likelihood_mixture_gaussians_batch(samples)\n",
    "            # tqdm.write(f'Log Likelihood mean: {lls.mean()} and std: {lls.std()}')\n",
    "            # plt.scatter(data[:, 0], data[:, 1], label='Original Data')\n",
    "            # plt.scatter(samples_np[:, 0], samples_np[:,1], label='Generated Samples')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "    \n",
    "    # plot log-likelihood mean across epochs with std shaded on top\n",
    "    return ll_mean, ll_std, epoch_ll\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model import MLP\n",
    "\n",
    "\n",
    "data = generate_mixture_gaussians(num_samples=4000)\n",
    "dataloader = DataLoader(TensorDataset(data), batch_size=500, shuffle=True)\n",
    "\n",
    "sde = VPSDE(num_steps, 0.1, 20, logarithmic_scheduling=False)\n",
    "score_net = MLP()\n",
    "ll_mean_alin, ll_std_alin, epoch_ll_alin = train_score_network(dataloader, score_net, sde, optimizer_str='adam', learning_rate=1e-4)\n",
    "\n",
    "\n",
    "sde = VPSDE(num_steps, 0.1, 20, logarithmic_scheduling=True)\n",
    "score_net = MLP()\n",
    "ll_mean_alog, ll_std_alog, epoch_ll_alog = train_score_network(dataloader, score_net, sde, optimizer_str='adam', learning_rate=1e-4)\n",
    "\n",
    "\n",
    "sde = VPSDE(num_steps, 0.1, 20, logarithmic_scheduling=False)\n",
    "score_net = MLP()\n",
    "ll_mean_plin, ll_std_plin, epoch_ll_plin = train_score_network(dataloader, score_net, sde, optimizer_str='prodigy')\n",
    "\n",
    "\n",
    "sde = VPSDE(num_steps, 0.1, 20, logarithmic_scheduling=True)\n",
    "score_net = MLP()\n",
    "ll_mean_plog, ll_std_plog, epoch_ll_plog  = train_score_network(dataloader, score_net, sde, optimizer_str='prodigy')\n",
    "\n",
    "# save the results\n",
    "import pickle\n",
    "results = {\n",
    "    'll_mean_alin': ll_mean_alin,\n",
    "    'll_std_alin': ll_std_alin,\n",
    "    'epoch_ll_alin': epoch_ll_alin,\n",
    "    'll_mean_alog': ll_mean_alog,\n",
    "    'll_std_alog': ll_std_alog,\n",
    "    'epoch_ll_alog': epoch_ll_alog,\n",
    "    'll_mean_plin': ll_mean_plin,\n",
    "    'll_std_plin': ll_std_plin,\n",
    "    'epoch_ll_plin': epoch_ll_plin,\n",
    "    'll_mean_plog': ll_mean_plog,\n",
    "    'll_std_plog': ll_std_plog,\n",
    "    'epoch_ll_plog': epoch_ll_plog\n",
    "}\n",
    "pickle.dump(results, open('llvsepochs.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "import pickle\n",
    "results = pickle.load(open('llvsepochs.pkl', 'rb'))\n",
    "ll_mean_alin = results['ll_mean_alin']\n",
    "ll_std_alin = results['ll_std_alin']\n",
    "epoch_ll_alin = results['epoch_ll_alin']\n",
    "ll_mean_alog = results['ll_mean_alog']\n",
    "ll_std_alog = results['ll_std_alog']\n",
    "epoch_ll_alog = results['epoch_ll_alog']\n",
    "ll_mean_plin = results['ll_mean_plin']\n",
    "ll_std_plin = results['ll_std_plin']\n",
    "epoch_ll_plin = results['epoch_ll_plin']\n",
    "ll_mean_plog = results['ll_mean_plog']\n",
    "ll_std_plog = results['ll_std_plog']\n",
    "epoch_ll_plog = results['epoch_ll_plog']\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# Using seaborn's style\n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "# width = 345\n",
    "\n",
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "}\n",
    "\n",
    "plt.rcParams.update(tex_fonts)\n",
    "\n",
    "def plot_ll_vs_epochs(ll_mean, ll_std, epoch_ll, label):\n",
    "    plt.plot(epoch_ll, ll_mean, label=label)\n",
    "    plt.fill_between(epoch_ll, np.array(ll_mean) - np.array(ll_std), np.array(ll_mean) + np.array(ll_std), alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_ll_vs_epochs(ll_mean_alin, ll_std_alin, epoch_ll_alin, 'Adam Linear')\n",
    "plot_ll_vs_epochs(ll_mean_alog, ll_std_alog, epoch_ll_alog, 'Adam Logarithmic')\n",
    "plot_ll_vs_epochs(ll_mean_plin, ll_std_plin, epoch_ll_plin, 'Prodigy Linear')\n",
    "plot_ll_vs_epochs(ll_mean_plog, ll_std_plog, epoch_ll_plog, 'Prodigy Logarithmic')\n",
    "\n",
    "\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(-8, 0)\n",
    "plt.ylabel('Mean Log Likelihood')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('ll_vs_epochs.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bridge_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}